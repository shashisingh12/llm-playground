# llm

A minimal, modular, and fully-trainable PyTorch implementation of a DeepSeek-style Large Language Model (LLM), built from scratch for research, learning, and experimentation.

## Features

- **From-scratch** DeepSeek-style transformer architecture
- Modular code for easy customization and research
- Supports Mixture-of-Experts (MoE) and multi-head latent attention (MLA)
- Fast training and inference with PyTorch
- HuggingFace tokenizer and safetensors checkpoint compatible
- Multi-GPU/distributed-ready (optional)
- Educational, clean, and hackable codebase
